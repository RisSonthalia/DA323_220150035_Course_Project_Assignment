{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44cebc06",
   "metadata": {},
   "source": [
    "# From Voice to Face: Exploring Speech2Face - Learning the Face Behind a Voice\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f570fc41",
   "metadata": {},
   "source": [
    "**Name:** Rishab Sonthalia  \n",
    "**Roll No.:** 220150035  \n",
    "**Course:** DA323\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b8ef95",
   "metadata": {},
   "source": [
    "## Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec19a8da",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "The connection between how someone looks and how they sound has always fascinated me. When we hear a voice on the phone or radio, we often unconsciously form mental images of the speaker. This natural human tendency to correlate vocal characteristics with physical appearance raises an intriguing question: can a machine learn to visualize a person's face just from hearing their voice?\n",
    "\n",
    "This question led me to explore the groundbreaking paper \"Speech2Face: Learning the Face Behind a Voice\" by Oh et al. The research tackles a fascinating cross-modal inference problem with applications ranging from accessibility technology to human-computer interaction. What makes this work particularly compelling is that it addresses a capability that humans naturally possess—inferring physical attributes from voice—and attempts to replicate it using deep learning techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc29879e",
   "metadata": {},
   "source": [
    "## Historical Perspective on Multimodal Learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be5e9cf",
   "metadata": {},
   "source": [
    "\n",
    "Speech2Face sits within a rich tradition of multimodal learning research that aims to bridge different sensory modalities using computational methods. Here's how it connects with past and current work:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9a830e",
   "metadata": {},
   "source": [
    "### Early Cross-Modal Research\n",
    "\n",
    "Early work in multimodal learning focused primarily on audio-visual correspondence, such as matching speech with lip movements. Research from the early 2000s explored statistical correlations between vocal features and physical characteristics, but these approaches typically relied on hand-crafted features and explicit modeling of specific attributes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec53855e",
   "metadata": {},
   "source": [
    "### The Rise of Self-Supervised Learning\n",
    "\n",
    "Around 2016-2017, Arandjelović and Zisserman introduced the Audio-Visual Correspondence (AVC) task, which leveraged the natural co-occurrence of audio and visual signals in videos as a form of self-supervision. This approach moved away from explicit modeling toward letting neural networks discover correlations between modalities directly from data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7d6b5a",
   "metadata": {},
   "source": [
    "### Recent Developments\n",
    "\n",
    "More recent work has explored increasingly complex cross-modal tasks:\n",
    "- **Sound Source Localization/Separation**: Identifying which objects in a scene are producing sounds (Ephrat et al., 2018)\n",
    "- **Cross-Modal Retrieval**: Searching for images based on audio queries and vice versa (Owens et al., 2016)\n",
    "- **Audio-Visual Representation Learning**: Creating joint embeddings that capture information from both modalities (Aytar et al., 2016)\n",
    "\n",
    "Speech2Face extends this tradition by tackling a particularly challenging cross-modal synthesis task: generating plausible face images directly from speech signals. Unlike earlier approaches that might have explicitly modeled discrete attributes (age, gender, etc.), Speech2Face attempts to learn these correlations directly from data in an end-to-end fashion.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d1f297",
   "metadata": {},
   "source": [
    "## Technical Deep-Dive: How Speech2Face Works"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9f0fa4",
   "metadata": {},
   "source": [
    "### Core Concept\n",
    "\n",
    "Speech2Face doesn't aim to recover the exact face of a speaker (an impossible task given that many different faces could produce the same voice). Instead, it attempts to reconstruct a face with dominant visual traits that correlate statistically with the input speech.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c905e25a",
   "metadata": {},
   "source": [
    "### Data Foundation\n",
    "\n",
    "The researchers utilized the AVSpeech dataset, which contains:\n",
    "- Millions of YouTube video clips with speaking faces\n",
    "- Over 100,000 different speakers\n",
    "- Diverse demographic representation (though with some imbalance)\n",
    "\n",
    "This large-scale dataset provided the natural co-occurrence of faces and voices needed for self-supervised learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515b8212",
   "metadata": {},
   "source": [
    "### Model Architecture\n",
    "\n",
    "The Speech2Face pipeline consists of two primary components:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c456e7",
   "metadata": {},
   "source": [
    "![Model Architecture](images/model_arch.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e819df",
   "metadata": {},
   "source": [
    "#### 1. Voice Encoder\n",
    "\n",
    "The trainable component that maps from speech to face feature vectors:\n",
    "\n",
    "```\n",
    "Input → Complex Spectrogram → CNN → 4096-D Face Feature Vector\n",
    "```\n",
    "\n",
    "Specifically, the voice encoder architecture consists of:\n",
    "- **Input**: Complex spectrogram (598 × 257 dimensions for 6-second audio, 2 channels representing real and imaginary components)\n",
    "- **Convolutional Blocks**: 8 convolutional layers with batch normalization and ReLU activation\n",
    "- **Pooling Strategy**: Max-pooling only along the temporal dimension to preserve frequency information (crucial for vocal characteristics)\n",
    "- **Final Layers**: Average pooling over time followed by two fully-connected layers producing a 4096-D face feature vector\n",
    "\n",
    "Key architectural insight: The model pools only in the temporal dimension, preserving frequency information that carries vocal characteristics, while linguistic information spans longer time durations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e12566f",
   "metadata": {},
   "source": [
    "#### Table 1. Voice Encoder Architecture\n",
    "\n",
    "| Layer        | CONV | CONV | CONV | CONV   | CONV | CONV   | CONV | CONV   | AVGPOOL |\n",
    "|--------------|------|------|------|--------|------|--------|------|--------|---------|\n",
    "| Input        | RELU | RELU | RELU | MAXPOOL| RELU | MAXPOOL| RELU | MAXPOOL| RELU    |\n",
    "|              | BN   | BN   | BN   | BN     | BN   | BN     | BN   | BN     | BN      |\n",
    "| Channels     | 2    | 64   | 64   | 128    | –    | 128    | –    | 128    | –       |\n",
    "| Stride       | –    | 1    | 1    | 1      | 2×1  | 1      | 2×1  | 1      | 2×1     |\n",
    "| Kernel Size  | –    | 4×4  | 4×4  | 4×4    | 2×1  | 4×4    | 2×1  | 4×4    | 2×1     |\n",
    "\n",
    "---\n",
    "\n",
    "| Layer        | CONV | RELU | FC   | FC   |\n",
    "|--------------|------|------|------|------|\n",
    "| Input        | RELU | RELU | RELU | RELU |\n",
    "|              | BN   |      |      |      |\n",
    "| Channels     | 256  | 512  | 512  | –    |\n",
    "| Stride       | 2×1  | 1    | 1    | 1    |\n",
    "| Kernel Size  | 4×4  | 4×4  | ∞×1  | 1×1  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e16040",
   "metadata": {},
   "source": [
    "**Notes:**\n",
    "- The input spectrogram dimensions are 598 × 257 (time × frequency) for a 6-second audio segment.\n",
    "\n",
    "- The two input channels correspond to the real and imaginary components of the spectrogram.\n",
    "\n",
    "- \"–\" indicates no operation or parameter for that cell.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f6ab57",
   "metadata": {},
   "source": [
    "#### 2. Face Decoder\n",
    "\n",
    "A pre-trained, fixed network that converts face features to canonical face images:\n",
    "- Takes the 4096-D feature vector and produces a frontal-facing, neutral expression face\n",
    "- Based on the decoder model from Cole et al.\n",
    "- Remains frozen during training to provide a stable target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264d4d53",
   "metadata": {},
   "source": [
    "### Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6104b133",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### Dataset\n",
    "- **AVSpeech**: Large-scale, \"in-the-wild\" audiovisual dataset\n",
    "- **Training/Test Split**: 1.7M / 0.15M spectra-face feature pairs\n",
    "\n",
    "---\n",
    "\n",
    "#### Audio Processing\n",
    "- **Segment length**: 6 seconds (shorter clips repeated)\n",
    "- **Sampling rate**: 16 kHz, single channel\n",
    "- **Spectrogram computation**:\n",
    "  - STFT with:\n",
    "    - 25ms Hann window\n",
    "    - 10ms hop length\n",
    "    - 512 FFT frequency bands\n",
    "- **Power-law compression**:\n",
    "  - Applied to real/imaginary components as:\n",
    "    - `sgn(S) × |S|^0.3`\n",
    "\n",
    "---\n",
    "\n",
    "#### Face Processing\n",
    "- **Face Detection**: CNN-based using Dlib\n",
    "- **Preprocessing**: Cropped and resized to 224×224 pixels\n",
    "- **Feature Extraction**: VGG-Face (4096-dimensional vector)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d31ca4",
   "metadata": {},
   "source": [
    "### Training Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85075524",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "The **voice encoder** is trained to **predict the face feature vector** `v_f` (from VGG-Face) using **speech input** to generate `v_s`.  \n",
    "The goal is to minimize the difference between the predicted and actual face features.\n",
    "\n",
    "---\n",
    "\n",
    "#### Loss Function Design\n",
    "\n",
    "A simple L1 loss was insufficient due to **unstable training**, so the final loss combines **three components**:\n",
    "\n",
    "$$\n",
    "L_{\\text{total}} = \\left\\| f_{\\text{dec}}(v_f) - f_{\\text{dec}}(v_s) \\right\\|_1 + \\lambda_1 \\left\\| \\frac{v_f}{\\|v_f\\|} - \\frac{v_s}{\\|v_s\\|} \\right\\|_2^2 + \\lambda_2 L_{\\text{distill}}\\left(f_{\\text{VGG}}(v_f), f_{\\text{VGG}}(v_s)\\right)\n",
    "$$\n",
    "\n",
    "\n",
    "Where:\n",
    "\n",
    "- **First term**: L1 distance between decoder activations\n",
    "- **Second term**: Normalized feature alignment with **λ₁ = 0.025**\n",
    "- **Third term**: **Knowledge distillation loss** with **λ₂ = 200**\n",
    "\n",
    "#### Knowledge Distillation Loss\n",
    "\n",
    "$$\n",
    "L_{\\text{distill}}(a, b) = -\\sum_i p(i)(a) \\log p(i)(b)\n",
    "\\\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "$$\n",
    "p(i)(a) = \\frac{\\exp(a_i / T)}{\\sum_j \\exp(a_j / T)}\n",
    "$$\n",
    "\n",
    "- **T = 2** (temperature parameter) smooths activation distributions\n",
    "\n",
    "---\n",
    "\n",
    "#### Hyperparameters & Optimization\n",
    "\n",
    "- **Framework**: TensorFlow\n",
    "- **Optimizer**: ADAM\n",
    "  - β₁ = 0.5\n",
    "  - ε = 1e-4\n",
    "- **Learning Rate**: 0.001 with exponential decay (rate = 0.95 every 10,000 iterations)\n",
    "- **Batch Size**: 8\n",
    "- **Training Duration**: 3 epochs\n",
    "\n",
    "---\n",
    "\n",
    "#### Loss Function Rationale\n",
    "\n",
    "- **Direct Feature Matching**: Basic L1 term ensures raw similarity\n",
    "- **Normalized Feature Alignment**: Directional alignment independent of magnitude\n",
    "- **Knowledge Distillation**: Ensures behavioral similarity using pre-trained VGG-Face features\n",
    "\n",
    "The weights **λ₁** and **λ₂** were carefully tuned to **balance gradient magnitudes** early in training.  \n",
    "This **multi-term loss design** significantly improved training **stability** and **reconstruction quality**, outperforming simple L1 loss strategies.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b9337d",
   "metadata": {},
   "source": [
    "## Results and Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7465e85f",
   "metadata": {},
   "source": [
    "\n",
    "![qualitative_results](images/qualitative_results.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ff6289",
   "metadata": {},
   "source": [
    "### 1. Qualitative Results\n",
    "\n",
    "The Speech2Face reconstructions successfully capture several key facial attributes:\n",
    "- Age category (young, middle-aged, elderly)\n",
    "- Gender\n",
    "- Ethnicity \n",
    "- Face/head shape (elongated vs. round)\n",
    "\n",
    "Visual inspection shows that while reconstructions appear somewhat like \"average faces,\" they still contain person-specific information that correlates with the voice input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e85563",
   "metadata": {},
   "source": [
    "\n",
    "![confusion_matrix_attributes](images/confusion_matrix_attributes.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745ffeb3",
   "metadata": {},
   "source": [
    "\n",
    "![avc_speech_data_stats](images/avc_speech_data_stats.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa14304",
   "metadata": {},
   "source": [
    "### 2. Demographic Attribute Analysis\n",
    "\n",
    "Using Face++ (a commercial facial attribute classifier), the researchers quantified agreement between original faces and Speech2Face reconstructions:\n",
    "\n",
    "**Gender Classification**:\n",
    "- 94% overall agreement between true images and reconstructions\n",
    "- 84% accuracy for males, 84% for females\n",
    "\n",
    "**Age Estimation**:\n",
    "- Strong diagonal pattern in confusion matrices\n",
    "- Best performance for younger and middle-aged adults\n",
    "\n",
    "**Ethnicity Classification**:\n",
    "- Variable performance across groups:\n",
    "  - White: 81% accuracy\n",
    "  - Asian: 76% accuracy\n",
    "  - Indian: 48% accuracy\n",
    "  - Black: 49% accuracy\n",
    "\n",
    "The lower performance on some ethnic groups correlates with their underrepresentation in the training data:\n",
    "- White: 50.2% of dataset\n",
    "- Asian: 28.9% of dataset\n",
    "- Indian: 12.1% of dataset\n",
    "- Black: 8.7% of dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4f07b5",
   "metadata": {},
   "source": [
    "\n",
    "![demographic_attributes](images/demographic_attributes.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08406d5d",
   "metadata": {},
   "source": [
    "### 3. Craniofacial Measurements\n",
    "\n",
    "One of the most fascinating findings is the statistically significant correlation between craniofacial measurements in true faces and Speech2Face reconstructions:\n",
    "\n",
    "| Face Measurement           | Correlation | p-value   |\n",
    "|---------------------------|-------------|-----------|\n",
    "| Upper lip height          | 0.16        | p < 0.001 |\n",
    "| Lateral upper lip heights | 0.26        | p < 0.001 |\n",
    "| Jaw width                 | 0.11        | p < 0.001 |\n",
    "| Nose height               | 0.14        | p < 0.001 |\n",
    "| Nose width                | 0.35        | p < 0.001 |\n",
    "| Labio oral region         | 0.17        | p < 0.001 |\n",
    "| Mandibular idx            | 0.20        | p < 0.001 |\n",
    "| Intercanthal idx          | 0.21        | p < 0.001 |\n",
    "| Nasal index               | 0.38        | p < 0.001 |\n",
    "| Vermilion height idx      | 0.29        | p < 0.001 |\n",
    "| Mouth face width idx      | 0.20        | p < 0.001 |\n",
    "| Nose area                 | 0.28        | p < 0.001 |\n",
    "| Random baseline           | 0.02        | –         |\n",
    "\n",
    "\n",
    "The strongest correlations appear in nose-related features, suggesting that nasal structure (which affects speech resonance) leaves detectable patterns in the voice that the model can pick up.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0afd0730",
   "metadata": {},
   "source": [
    "### 4. Cross-Modal Retrieval Performance\n",
    "\n",
    "The researchers tested how well Speech2Face features could retrieve the correct face from a database of 5,000 images:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385ae842",
   "metadata": {},
   "source": [
    "| Length     | cos (deg)         | L2              | L1              |\n",
    "|------------|-------------------|------------------|------------------|\n",
    "| 3 seconds  | 48.43 ± 6.01      | 0.19 ± 0.03     | 9.81 ± 1.74     |\n",
    "| 6 seconds  | 45.75 ± 5.09      | 0.18 ± 0.02     | 9.42 ± 1.54     |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09cc6d32",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "![stf_3s_6s_comparison](images/stf_3s_6s_comparison.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1e57cb",
   "metadata": {},
   "source": [
    "| Duration | Metric | R@1  | R@2  | R@5  | R@10 |\n",
    "|----------|--------|------|------|------|------|\n",
    "| 3 sec    | L2     | 5.86 | 10.02| 18.98| 28.92|\n",
    "| 3 sec    | L1     | 6.22 | 9.92 | 18.94| 28.70|\n",
    "| 3 sec    | cos    | 8.54 | 13.64| 24.80| 38.54|\n",
    "| 6 sec    | L2     | 8.28 | 13.66| 24.66| 35.84|\n",
    "| 6 sec    | L1     | 8.34 | 13.70| 24.66| 36.22|\n",
    "| 6 sec    | cos    |10.92 | 17.00| 30.60| 45.82|\n",
    "| Random   | –      | 1.00 | 2.00 | 5.00 | 10.00|\n",
    "\n",
    "**S2F→Face retrieval performance.Measure retrieval\n",
    "performance by recall at K (R@K, in %), which indicates the\n",
    "chance of retrieving the true image of a speaker within the top-K\n",
    "results.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2525594a",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "![feature_similarity_2](images/feature_similarity_2.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7256d398",
   "metadata": {},
   "source": [
    "- **Direct Feature Comparison**\n",
    "\n",
    "When comparing the model's **predicted face features** to the **real person's face features**, **longer audio clips** (6 seconds vs. 3 seconds) consistently improved accuracy.\n",
    "\n",
    "---\n",
    "\n",
    "- **Face Retrieval Test**\n",
    "\n",
    "    In a more practical test, the researchers:\n",
    "\n",
    "    1. Used the model to **predict face features from a voice**\n",
    "    2. **Searched a database** of 5,000 faces to find matches\n",
    "    3. Measured how often the **correct face appeared in the top results**\n",
    "\n",
    "---\n",
    "\n",
    "- **Results**\n",
    "\n",
    "    -  With **6 seconds** of speech:\n",
    "          - Correct face was the **#1 match** about **11%** of the time\n",
    "          - Correct face appeared in the **top 5** results about **31%** of the time\n",
    "          - Correct face appeared in the **top 10** results about **46%** of the time\n",
    "    - Feature similarity improved (cosine distance: 45.75° vs. 48.43°)\n",
    "    - Retrieval performance increased (~2-6% improvement across metrics)\n",
    "    - Visual quality of reconstructions showed noticeable enhancement\n",
    "    - **Random chance** for a perfect match (1 in 5,000) would be just **0.02%**, so even **1%** is a strong baseline — these results **significantly outperform random**.\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ab9988",
   "metadata": {},
   "source": [
    "### 5. **Conclusion**\n",
    "These results demonstrate that:\n",
    "\n",
    "- **Voices carry meaningful information about facial appearance**\n",
    "- Even when the exact match wasn't found, retrieved faces shared **similar traits** (age, gender, ethnicity, facial structure)\n",
    "- **Deep learning** can effectively capture these **cross-modal relationships**\n",
    "\n",
    "#### **Our voices reveal more about our **physical appearance** than we might expect!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9ba996",
   "metadata": {},
   "source": [
    "### Training Components Analysis\n",
    "\n",
    "The ablation studies revealed important insights about training methodology:\n",
    "- **Batch Normalization**: Significantly improved convergence speed and stability\n",
    "- **Loss Function**: The full composite loss produced much sharper, more accurate reconstructions than pixel-level loss alone\n",
    "- **Audio Duration**: Models trained with 6-second clips consistently outperformed those trained with 3-second clips\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8148dabf",
   "metadata": {},
   "source": [
    "## Reflections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8798dc85",
   "metadata": {},
   "source": [
    "Several aspects of this work were particularly surprising:\n",
    "\n",
    "1. **Craniofacial Correlations**: The significant correlation between specific facial measurements and voice was unexpected. The fact that nasal features showed the strongest correlation makes physiological sense (nasal cavity shapes affect voice resonance), but it's remarkable that the model discovered this relationship without explicit guidance.\n",
    "\n",
    "2. **Retrieval Performance**: The ability to retrieve the correct face from a database of 5,000 faces at a rate of 10.92% (R@1) is surprisingly good considering the one-to-many relationship between voices and faces. This suggests the voice contains more identifying information than I initially expected.\n",
    "\n",
    "3. **Training Stability Challenges**: The fact that a simple L1 loss between features was insufficient for stable training highlights the complexity of cross-modal learning. The need for a composite loss function with knowledge distillation reveals the subtlety of the optimization landscape.\n",
    "\n",
    "4. **Demographic Bias Effects**: While not entirely surprising, the clear correlation between dataset representation and reconstruction accuracy for different ethnic groups emphasizes the importance of balanced training data in multimodal systems.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8a1ad1",
   "metadata": {},
   "source": [
    "### Scope for Improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb99055",
   "metadata": {},
   "source": [
    "This work opens several promising avenues for further research:\n",
    "\n",
    "1. **Balanced Data Representation**: Training on a more demographically balanced dataset would likely improve performance across all groups. The paper acknowledges this limitation, and addressing it would be a straightforward yet impactful improvement.\n",
    "\n",
    "2. **Temporal Dynamics**: The current model uses average pooling over time to create a single face representation. Incorporating temporal dynamics of speech might capture additional information about facial structure and expressions.\n",
    "\n",
    "3. **Multiple Face Hypotheses**: Since voice-to-face is a one-to-many mapping, generating multiple plausible face hypotheses (perhaps using variational or generative approaches) could better represent the inherent ambiguity.\n",
    "\n",
    "4. **Additional Modalities**: Incorporating linguistic content analysis alongside acoustic features might improve reconstruction, as certain speech patterns correlate with cultural or regional facial characteristics.\n",
    "\n",
    "5. **Ethical Safeguards**: While the paper addresses ethical considerations, developing more robust privacy guarantees would be valuable for real-world applications.\n",
    "\n",
    "6. **Dynamic Face Reconstruction**: Moving beyond static, neutral faces to reconstruct plausible facial movements during speech would be a fascinating extension."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a134085",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. Oh, T. H., Dekel, T., Kim, C., Mosseri, I., Freeman, W. T., Rubinstein, M., & Matusik, W. (2019). Speech2Face: Learning the Face Behind a Voice. *IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)*.\n",
    "\n",
    "2. Arandjelović, R., & Zisserman, A. (2017). Look, Listen and Learn. *IEEE International Conference on Computer Vision (ICCV)*.\n",
    "\n",
    "3. Ephrat, A., Mosseri, I., Lang, O., Dekel, T., Wilson, K., Hassidim, A., Freeman, W. T., & Rubinstein, M. (2018). Looking to Listen at the Cocktail Party: A Speaker-Independent Audio-Visual Model for Speech Separation. *ACM Transactions on Graphics (TOG)*.\n",
    "\n",
    "4. Cole, F., Belanger, D., Krishnan, D., Sarna, A., Mosseri, I., & Freeman, W. T. (2017). Face Synthesis from Facial Identity Features. *IEEE Conference on Computer Vision and Pattern Recognition (CVPR)*.\n",
    "\n",
    "5. Aytar, Y., Vondrick, C., & Torralba, A. (2016). SoundNet: Learning Sound Representations from Unlabeled Video. *Advances in Neural Information Processing Systems (NIPS)*.\n",
    "\n",
    "6. Liu, M.-Y., & Tuzel, O. (2016). Coupled Generative Adversarial Networks. *Advances in Neural Information Processing Systems (NIPS)*.\n",
    "\n",
    "7. Owens, A., Wu, J., McDermott, J. H., Freeman, W. T., & Torralba, A. (2016). Ambient Sound Provides Supervision for Visual Learning. *European Conference on Computer Vision (ECCV)*.\n",
    "\n",
    "8. [Speech2Face GitHub Repository](https://speech2face.github.io/) (Official project page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efbc176",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
